"""
Compression Operatorsï¼ˆå‹ç¼©ï¼‰

è®ºæ–‡æ ¸å¿ƒæŠ€æœ¯ï¼š
- å‡å°‘æ£€ç´¢å†…å®¹ä»¥æœ€å°åŒ–å™ªéŸ³
- å‹ç¼©ä¸Šä¸‹æ–‡ä»¥é€‚åº” LLM çª—å£é™åˆ¶
- ä¿ç•™å…³é”®ä¿¡æ¯ï¼Œå»é™¤å†—ä½™
"""

from typing import List, Dict, Any
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_qwq import ChatQwen
from .base import BasePostRetrievalOperator


class ContextCompressionOperator(BasePostRetrievalOperator):
    """
    Context Compression æ“ä½œå™¨ï¼ˆä¸Šä¸‹æ–‡å‹ç¼©ï¼‰

    åŠŸèƒ½ï¼š
    - å‹ç¼©æ–‡æ¡£å†…å®¹
    - åªä¿ç•™ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„éƒ¨åˆ†
    - å‡å°‘ token ä½¿ç”¨

    åº”ç”¨åœºæ™¯ï¼š
    - é•¿æ–‡æ¡£éœ€è¦ç²¾ç®€
    - Token é¢„ç®—æœ‰é™
    - éœ€è¦æå–å…³é”®ä¿¡æ¯
    """

    def __init__(self, config: Dict[str, Any] = None):
        super().__init__(config)
        self.max_tokens = self.config.get("max_tokens", 200)  # æ¯ä¸ªæ–‡æ¡£æœ€å¤§ token æ•°
        self.compression_ratio = self.config.get("compression_ratio", 0.5)  # å‹ç¼©æ¯”ä¾‹

    def process(self, documents: List[Document], query: str = None) -> List[Document]:
        """
        å‹ç¼©æ–‡æ¡£å†…å®¹

        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            query: åŸå§‹æŸ¥è¯¢

        Returns:
            å‹ç¼©åçš„æ–‡æ¡£åˆ—è¡¨
        """
        if not documents:
            return []

        print(f"ğŸ—œï¸  Context Compression: å‹ç¼© {len(documents)} ä¸ªæ–‡æ¡£...")

        compressed_docs = []
        total_original_tokens = 0
        total_compressed_tokens = 0

        for doc in documents:
            original_length = len(doc.page_content)
            total_original_tokens += original_length

            # å‹ç¼©æ–‡æ¡£
            compressed_content = self._compress_content(doc.page_content, query)

            # åˆ›å»ºæ–°æ–‡æ¡£
            compressed_doc = Document(
                page_content=compressed_content,
                metadata=doc.metadata.copy()
            )
            compressed_doc.metadata["original_length"] = original_length
            compressed_doc.metadata["compressed_length"] = len(compressed_content)
            compressed_doc.metadata["compression_ratio"] = len(compressed_content) / original_length if original_length > 0 else 0

            compressed_docs.append(compressed_doc)
            total_compressed_tokens += len(compressed_content)

        actual_ratio = total_compressed_tokens / total_original_tokens if total_original_tokens > 0 else 0
        print(f"   âœ“ å‹ç¼©å®Œæˆ")
        print(f"   åŸå§‹: {total_original_tokens} tokens")
        print(f"   å‹ç¼©å: {total_compressed_tokens} tokens")
        print(f"   å‹ç¼©ç‡: {actual_ratio:.2%}")

        return compressed_docs

    def _compress_content(self, content: str, query: str = None) -> str:
        """
        å‹ç¼©æ–‡æ¡£å†…å®¹

        ç®€å•ç­–ç•¥ï¼š
        1. å¦‚æœæœ‰æŸ¥è¯¢ï¼Œæå–ä¸æŸ¥è¯¢ç›¸å…³çš„å¥å­
        2. å¦åˆ™ï¼Œä¿ç•™å‰ N ä¸ªå­—ç¬¦

        Args:
            content: åŸå§‹å†…å®¹
            query: æŸ¥è¯¢

        Returns:
            å‹ç¼©åçš„å†…å®¹
        """
        # è®¡ç®—ç›®æ ‡é•¿åº¦
        target_length = int(len(content) * self.compression_ratio)
        target_length = max(target_length, self.max_tokens)

        if len(content) <= target_length:
            return content

        if query:
            # æå–ä¸æŸ¥è¯¢ç›¸å…³çš„å¥å­
            sentences = content.split('ã€‚')
            query_words = set(query.lower().split())

            # è®¡ç®—æ¯ä¸ªå¥å­çš„ç›¸å…³æ€§
            scored_sentences = []
            for sent in sentences:
                if not sent.strip():
                    continue
                sent_words = set(sent.lower().split())
                overlap = len(query_words & sent_words)
                scored_sentences.append((sent, overlap))

            # æŒ‰ç›¸å…³æ€§æ’åº
            scored_sentences.sort(key=lambda x: x[1], reverse=True)

            # é€‰æ‹©æœ€ç›¸å…³çš„å¥å­ç›´åˆ°è¾¾åˆ°ç›®æ ‡é•¿åº¦
            compressed = []
            current_length = 0

            for sent, score in scored_sentences:
                if current_length + len(sent) <= target_length:
                    compressed.append(sent)
                    current_length += len(sent)
                else:
                    break

            return 'ã€‚'.join(compressed) + 'ã€‚' if compressed else content[:target_length]
        else:
            # ç®€å•æˆªæ–­
            return content[:target_length] + "..."


class SummaryCompressionOperator(BasePostRetrievalOperator):
    """
    Summary Compression æ“ä½œå™¨ï¼ˆæ‘˜è¦å‹ç¼©ï¼‰

    åŠŸèƒ½ï¼š
    - ä½¿ç”¨ LLM ç”Ÿæˆæ–‡æ¡£æ‘˜è¦
    - ä¿ç•™å…³é”®ä¿¡æ¯
    - æ›´æ™ºèƒ½çš„å‹ç¼©æ–¹å¼

    åº”ç”¨åœºæ™¯ï¼š
    - éœ€è¦é«˜è´¨é‡å‹ç¼©
    - ä¿æŒè¯­ä¹‰å®Œæ•´æ€§
    - å¯¹è´¨é‡è¦æ±‚é«˜
    """

    def __init__(self, config: Dict[str, Any] = None):
        super().__init__(config)
        self.model = self.config.get("model", "qwen-plus")
        self.temperature = self.config.get("temperature", 0.3)
        self.max_summary_length = self.config.get("max_summary_length", 200)

        # åˆå§‹åŒ– LLM
        self.llm = ChatQwen(
            model=self.model,
            temperature=self.temperature,
        )

    def process(self, documents: List[Document], query: str = None) -> List[Document]:
        """
        é€šè¿‡æ‘˜è¦å‹ç¼©æ–‡æ¡£

        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            query: åŸå§‹æŸ¥è¯¢

        Returns:
            æ‘˜è¦åçš„æ–‡æ¡£åˆ—è¡¨
        """
        if not documents:
            return []

        print(f"ğŸ“ Summary Compression: ç”Ÿæˆ {len(documents)} ä¸ªæ–‡æ¡£çš„æ‘˜è¦...")

        summarized_docs = []

        for i, doc in enumerate(documents, 1):
            print(f"   å¤„ç†æ–‡æ¡£ {i}/{len(documents)}...")

            # ç”Ÿæˆæ‘˜è¦
            summary = self._generate_summary(doc.page_content, query)

            # åˆ›å»ºæ–°æ–‡æ¡£
            summarized_doc = Document(
                page_content=summary,
                metadata=doc.metadata.copy()
            )
            summarized_doc.metadata["original_length"] = len(doc.page_content)
            summarized_doc.metadata["summary_length"] = len(summary)
            summarized_doc.metadata["is_summary"] = True

            summarized_docs.append(summarized_doc)

        print(f"   âœ“ æ‘˜è¦ç”Ÿæˆå®Œæˆ")

        return summarized_docs

    def _generate_summary(self, content: str, query: str = None) -> str:
        """
        ç”Ÿæˆæ–‡æ¡£æ‘˜è¦

        Args:
            content: åŸå§‹å†…å®¹
            query: æŸ¥è¯¢ï¼ˆç”¨äºå¼•å¯¼æ‘˜è¦ï¼‰

        Returns:
            æ‘˜è¦æ–‡æœ¬
        """
        # å¦‚æœå†…å®¹å·²ç»å¾ˆçŸ­ï¼Œç›´æ¥è¿”å›
        if len(content) <= self.max_summary_length:
            return content

        if query:
            prompt = ChatPromptTemplate.from_messages([
                ("system", """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£æ‘˜è¦åŠ©æ‰‹ã€‚è¯·æ ¹æ®æŸ¥è¯¢ç”Ÿæˆæ–‡æ¡£çš„ç®€æ´æ‘˜è¦ã€‚

è¦æ±‚ï¼š
1. é‡ç‚¹å…³æ³¨ä¸æŸ¥è¯¢ç›¸å…³çš„ä¿¡æ¯
2. ä¿ç•™å…³é”®äº‹å®å’Œæ•°æ®
3. æ‘˜è¦é•¿åº¦ä¸è¶…è¿‡ {max_length} å­—
4. ä¿æŒå®¢è§‚å‡†ç¡®
5. ç›´æ¥è¾“å‡ºæ‘˜è¦ï¼Œä¸éœ€è¦å‰ç¼€

ç¤ºä¾‹ï¼š
æŸ¥è¯¢ï¼šç¾å›½ç§‘æŠ€è¡Œä¸šç°çŠ¶
æ–‡æ¡£ï¼š[é•¿æ–‡æ¡£å†…å®¹]
æ‘˜è¦ï¼šç¾å›½ç§‘æŠ€è¡Œä¸šè¿‘æœŸå‡ºç°å¤§è§„æ¨¡è£å‘˜æ½®ï¼Œå¤šå®¶å·¨å¤´å…¬å¸å‰Šå‡å‘˜å·¥ã€‚ä¸»è¦åŸå› åŒ…æ‹¬ç–«æƒ…æœŸé—´è¿‡åº¦æ‰©å¼ ã€AIæŠ•èµ„å·¨å¤§ä½†ç›ˆåˆ©ä¸åŠé¢„æœŸç­‰ã€‚"""),
                ("human", "æŸ¥è¯¢ï¼š{query}\n\næ–‡æ¡£ï¼š{content}\n\næ‘˜è¦ï¼š"),
            ])
        else:
            prompt = ChatPromptTemplate.from_messages([
                ("system", """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£æ‘˜è¦åŠ©æ‰‹ã€‚è¯·ç”Ÿæˆæ–‡æ¡£çš„ç®€æ´æ‘˜è¦ã€‚

è¦æ±‚ï¼š
1. æå–æ ¸å¿ƒä¿¡æ¯å’Œå…³é”®è¦ç‚¹
2. æ‘˜è¦é•¿åº¦ä¸è¶…è¿‡ {max_length} å­—
3. ä¿æŒå®¢è§‚å‡†ç¡®
4. ç›´æ¥è¾“å‡ºæ‘˜è¦ï¼Œä¸éœ€è¦å‰ç¼€"""),
                ("human", "æ–‡æ¡£ï¼š{content}\n\næ‘˜è¦ï¼š"),
            ])

        chain = prompt | self.llm | StrOutputParser()

        try:
            summary = chain.invoke({
                "query": query,
                "content": content[:2000],  # é™åˆ¶è¾“å…¥é•¿åº¦
                "max_length": self.max_summary_length
            }).strip()

            return summary
        except Exception as e:
            print(f"   âš ï¸  æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}ï¼Œä½¿ç”¨æˆªæ–­")
            return content[:self.max_summary_length] + "..."


class TokenCompressionOperator(BasePostRetrievalOperator):
    """
    Token Compression æ“ä½œå™¨ï¼ˆToken çº§å‹ç¼©ï¼‰

    åŠŸèƒ½ï¼š
    - ç§»é™¤ä¸é‡è¦çš„ tokenï¼ˆå† è¯ã€ä»‹è¯ç­‰ï¼‰
    - ä¿ç•™å…³é”®è¯å’Œå®ä½“
    - ç±»ä¼¼ LLMLingua çš„æ€æƒ³

    åº”ç”¨åœºæ™¯ï¼š
    - æè‡´å‹ç¼©éœ€æ±‚
    - Token é¢„ç®—éå¸¸æœ‰é™
    - å…³é”®è¯æ£€ç´¢åœºæ™¯
    """

    def __init__(self, config: Dict[str, Any] = None):
        super().__init__(config)
        self.compression_ratio = self.config.get("compression_ratio", 0.6)

        # ä¸­æ–‡åœç”¨è¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
        self.stopwords = set([
            'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'å’Œ', 'ä¸', 'ç­‰', 'åŠ', 'ä¹Ÿ', 'éƒ½',
            'å°±', 'è€Œ', 'å°†', 'è¢«', 'æŠŠ', 'ç»™', 'ä»', 'å‘', 'åˆ°', 'ä¸º',
            'ä»¥', 'äº', 'å¯¹', 'ç€', 'ä¹‹', 'è¿™', 'é‚£', 'æœ‰', 'ä¸ª', 'å’Œ',
        ])

    def process(self, documents: List[Document], query: str = None) -> List[Document]:
        """
        Token çº§å‹ç¼©

        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            query: åŸå§‹æŸ¥è¯¢

        Returns:
            å‹ç¼©åçš„æ–‡æ¡£åˆ—è¡¨
        """
        if not documents:
            return []

        print(f"ğŸ”¤ Token Compression: Token çº§å‹ç¼© {len(documents)} ä¸ªæ–‡æ¡£...")

        compressed_docs = []
        total_original = 0
        total_compressed = 0

        for doc in documents:
            original_length = len(doc.page_content)
            total_original += original_length

            # å‹ç¼©
            compressed_content = self._compress_tokens(doc.page_content, query)

            # åˆ›å»ºæ–°æ–‡æ¡£
            compressed_doc = Document(
                page_content=compressed_content,
                metadata=doc.metadata.copy()
            )
            compressed_doc.metadata["original_length"] = original_length
            compressed_doc.metadata["compressed_length"] = len(compressed_content)

            compressed_docs.append(compressed_doc)
            total_compressed += len(compressed_content)

        actual_ratio = total_compressed / total_original if total_original > 0 else 0
        print(f"   âœ“ Token å‹ç¼©å®Œæˆ")
        print(f"   å‹ç¼©ç‡: {actual_ratio:.2%}")

        return compressed_docs

    def _compress_tokens(self, content: str, query: str = None) -> str:
        """
        ç§»é™¤ä¸é‡è¦çš„ token

        Args:
            content: åŸå§‹å†…å®¹
            query: æŸ¥è¯¢

        Returns:
            å‹ç¼©åçš„å†…å®¹
        """
        # ç®€å•åˆ†è¯ï¼ˆæŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹ï¼‰
        import re
        tokens = re.findall(r'[\w]+|[^\w\s]', content)

        # æå–æŸ¥è¯¢å…³é”®è¯
        query_keywords = set(query.split()) if query else set()

        # è¯„ä¼°æ¯ä¸ª token çš„é‡è¦æ€§
        important_tokens = []

        for token in tokens:
            # ä¿ç•™æ¡ä»¶ï¼š
            # 1. ä¸æ˜¯åœç”¨è¯
            # 2. é•¿åº¦ > 1
            # 3. æˆ–è€…åœ¨æŸ¥è¯¢ä¸­å‡ºç°
            if (
                token not in self.stopwords and
                len(token) > 1
            ) or token in query_keywords:
                important_tokens.append(token)

        # é‡ç»„æ–‡æœ¬
        compressed = ''.join(important_tokens)

        # å¦‚æœå‹ç¼©ç‡ä¸å¤Ÿï¼Œè¿›ä¸€æ­¥å‹ç¼©
        target_length = int(len(content) * self.compression_ratio)
        if len(compressed) > target_length:
            compressed = compressed[:target_length]

        return compressed


class AdaptiveCompressionOperator(BasePostRetrievalOperator):
    """
    Adaptive Compression æ“ä½œå™¨ï¼ˆè‡ªé€‚åº”å‹ç¼©ï¼‰

    åŠŸèƒ½ï¼š
    - æ ¹æ®æ–‡æ¡£é•¿åº¦å’Œç›¸å…³æ€§åŠ¨æ€é€‰æ‹©å‹ç¼©ç­–ç•¥
    - é«˜ç›¸å…³ + çŸ­æ–‡æ¡£ -> ä¸å‹ç¼©
    - é«˜ç›¸å…³ + é•¿æ–‡æ¡£ -> æ‘˜è¦å‹ç¼©
    - ä½ç›¸å…³æ–‡æ¡£ -> å¼ºåŠ›å‹ç¼©æˆ–ç§»é™¤

    åº”ç”¨åœºæ™¯ï¼š
    - æ™ºèƒ½å‹ç¼©
    - å¹³è¡¡è´¨é‡å’Œæ•ˆç‡
    """

    def __init__(self, config: Dict[str, Any] = None):
        super().__init__(config)
        self.short_threshold = self.config.get("short_threshold", 200)
        self.long_threshold = self.config.get("long_threshold", 1000)

    def process(self, documents: List[Document], query: str = None) -> List[Document]:
        """
        è‡ªé€‚åº”å‹ç¼©æ–‡æ¡£

        Args:
            documents: æ–‡æ¡£åˆ—è¡¨ï¼ˆå‡è®¾æŒ‰ç›¸å…³æ€§æ’åºï¼‰
            query: åŸå§‹æŸ¥è¯¢

        Returns:
            å‹ç¼©åçš„æ–‡æ¡£åˆ—è¡¨
        """
        if not documents:
            return []

        print(f"ğŸ¯ Adaptive Compression: è‡ªé€‚åº”å‹ç¼© {len(documents)} ä¸ªæ–‡æ¡£...")

        processed_docs = []

        for i, doc in enumerate(documents):
            doc_length = len(doc.page_content)

            # æ ¹æ®ä½ç½®åˆ¤æ–­ç›¸å…³æ€§ï¼ˆå‰é¢çš„æ›´ç›¸å…³ï¼‰
            is_highly_relevant = i < len(documents) * 0.3

            # å†³å®šå‹ç¼©ç­–ç•¥
            if is_highly_relevant and doc_length <= self.short_threshold:
                # é«˜ç›¸å…³ + çŸ­æ–‡æ¡£ -> ä¸å‹ç¼©
                print(f"   æ–‡æ¡£ {i+1}: ä¿æŒåŸæ ·ï¼ˆé«˜ç›¸å…³ä¸”ç®€çŸ­ï¼‰")
                processed_docs.append(doc)

            elif is_highly_relevant and doc_length > self.long_threshold:
                # é«˜ç›¸å…³ + é•¿æ–‡æ¡£ -> ä¸­ç­‰å‹ç¼©
                print(f"   æ–‡æ¡£ {i+1}: ä¸­ç­‰å‹ç¼©ï¼ˆé«˜ç›¸å…³ä½†è¾ƒé•¿ï¼‰")
                compressed_content = doc.page_content[:int(doc_length * 0.7)]
                compressed_doc = Document(
                    page_content=compressed_content,
                    metadata=doc.metadata.copy()
                )
                processed_docs.append(compressed_doc)

            elif not is_highly_relevant and doc_length > self.short_threshold:
                # ä½ç›¸å…³ + é•¿æ–‡æ¡£ -> å¼ºåŠ›å‹ç¼©
                print(f"   æ–‡æ¡£ {i+1}: å¼ºåŠ›å‹ç¼©ï¼ˆç›¸å…³æ€§è¾ƒä½ï¼‰")
                compressed_content = doc.page_content[:int(doc_length * 0.4)]
                compressed_doc = Document(
                    page_content=compressed_content,
                    metadata=doc.metadata.copy()
                )
                processed_docs.append(compressed_doc)

            else:
                # å…¶ä»–æƒ…å†µ -> é€‚åº¦å‹ç¼©
                print(f"   æ–‡æ¡£ {i+1}: é€‚åº¦å‹ç¼©")
                compressed_content = doc.page_content[:int(doc_length * 0.6)]
                compressed_doc = Document(
                    page_content=compressed_content,
                    metadata=doc.metadata.copy()
                )
                processed_docs.append(compressed_doc)

        print(f"   âœ“ è‡ªé€‚åº”å‹ç¼©å®Œæˆ")

        return processed_docs

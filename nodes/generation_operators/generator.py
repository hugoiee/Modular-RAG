"""
LLM Generator Operatorsï¼ˆLLM ç”Ÿæˆå™¨ï¼‰

æ”¯æŒä¸åŒçš„ LLM å’Œç”Ÿæˆç­–ç•¥
"""

from typing import List, Dict, Any
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_qwq import ChatQwen
from .base import BaseGenerationOperator


class LLMGeneratorOperator(BaseGenerationOperator):
    """
    åŸºç¡€ LLM Generator æ“ä½œå™¨

    åŠŸèƒ½ï¼š
    - è°ƒç”¨ LLM ç”Ÿæˆç­”æ¡ˆ
    - æ”¯æŒå¤šç§æ¨¡å‹é…ç½®
    - å¤„ç†ç”Ÿæˆå‚æ•°

    åº”ç”¨åœºæ™¯ï¼š
    - æ ‡å‡†æ–‡æœ¬ç”Ÿæˆ
    - RAG é—®ç­”
    """

    def __init__(self, config: Dict[str, Any] = None):
        super().__init__(config)
        self.model = self.config.get("model", "qwen-plus")
        self.temperature = self.config.get("temperature", 0.7)
        self.max_tokens = self.config.get("max_tokens", 2000)
        self.top_p = self.config.get("top_p", 0.9)

        # åˆå§‹åŒ– LLM
        self.llm = ChatQwen(
            model=self.model,
            temperature=self.temperature,
            max_tokens=self.max_tokens,
            top_p=self.top_p,
        )

    def execute(self, query: str, context: List[Document] = None, **kwargs) -> str:
        """
        ç”Ÿæˆç­”æ¡ˆ

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context: ä¸Šä¸‹æ–‡æ–‡æ¡£
            **kwargs: é¢å¤–å‚æ•°ï¼ˆå¦‚ prompt_templateï¼‰

        Returns:
            ç”Ÿæˆçš„ç­”æ¡ˆ
        """
        # è·å–æç¤ºï¼ˆå¯èƒ½ä» kwargs ä¼ å…¥ï¼‰
        prompt_text = kwargs.get("prompt", None)

        if prompt_text is None:
            # ä½¿ç”¨é»˜è®¤æç¤ºæ ¼å¼
            prompt_text = self._build_default_prompt(query, context)

        # åˆ›å»º prompt template
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚"),
            ("human", prompt_text)
        ])

        # ç”Ÿæˆ
        chain = prompt_template | self.llm | StrOutputParser()

        try:
            answer = chain.invoke({})
            return answer.strip()
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}"

    def _build_default_prompt(self, query: str, context: List[Document]) -> str:
        """æ„å»ºé»˜è®¤æç¤º"""
        if context:
            context_text = "\n\n".join([f"[æ–‡æ¡£ {i+1}]\n{doc.page_content}" for i, doc in enumerate(context)])
            return f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ï¼š

ä¸Šä¸‹æ–‡ï¼š
{context_text}

é—®é¢˜ï¼š{query}

ç­”æ¡ˆï¼š"""
        else:
            return f"é—®é¢˜ï¼š{query}\n\nç­”æ¡ˆï¼š"


class StreamGeneratorOperator(BaseGenerationOperator):
    """
    Stream Generator æ“ä½œå™¨ï¼ˆæµå¼ç”Ÿæˆï¼‰

    åŠŸèƒ½ï¼š
    - æµå¼è¾“å‡ºç­”æ¡ˆ
    - å®æ—¶æ˜¾ç¤ºç”Ÿæˆè¿‡ç¨‹
    - æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ

    åº”ç”¨åœºæ™¯ï¼š
    - éœ€è¦å®æ—¶åé¦ˆ
    - é•¿æ–‡æœ¬ç”Ÿæˆ
    - äº¤äº’å¼åº”ç”¨
    """

    def __init__(self, config: Dict[str, Any] = None):
        super().__init__(config)
        self.model = self.config.get("model", "qwen-plus")
        self.temperature = self.config.get("temperature", 0.7)
        self.max_tokens = self.config.get("max_tokens", 2000)

        # åˆå§‹åŒ– LLMï¼ˆå¯ç”¨æµå¼ï¼‰
        self.llm = ChatQwen(
            model=self.model,
            temperature=self.temperature,
            max_tokens=self.max_tokens,
            streaming=True,
        )

    def execute(self, query: str, context: List[Document] = None, **kwargs) -> str:
        """
        æµå¼ç”Ÿæˆç­”æ¡ˆ

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context: ä¸Šä¸‹æ–‡æ–‡æ¡£
            **kwargs: é¢å¤–å‚æ•°

        Returns:
            å®Œæ•´çš„ç”Ÿæˆç­”æ¡ˆ
        """
        prompt_text = kwargs.get("prompt", self._build_default_prompt(query, context))

        prompt_template = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚"),
            ("human", prompt_text)
        ])

        print("\nğŸ”„ å¼€å§‹æµå¼ç”Ÿæˆ...")
        print("-" * 60)

        # æ”¶é›†æµå¼è¾“å‡º
        full_response = []

        try:
            for chunk in self.llm.stream(prompt_template.format_messages()):
                content = chunk.content
                print(content, end="", flush=True)
                full_response.append(content)

            print("\n" + "-" * 60)
            print("âœ… ç”Ÿæˆå®Œæˆ\n")

            return "".join(full_response).strip()

        except Exception as e:
            print(f"\nâŒ æµå¼ç”Ÿæˆå¤±è´¥: {e}")
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}"

    def _build_default_prompt(self, query: str, context: List[Document]) -> str:
        """æ„å»ºé»˜è®¤æç¤º"""
        if context:
            context_text = "\n\n".join([f"[æ–‡æ¡£ {i+1}]\n{doc.page_content}" for i, doc in enumerate(context)])
            return f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ï¼š

ä¸Šä¸‹æ–‡ï¼š
{context_text}

é—®é¢˜ï¼š{query}

ç­”æ¡ˆï¼š"""
        else:
            return f"é—®é¢˜ï¼š{query}\n\nç­”æ¡ˆï¼š"


class EnsembleGeneratorOperator(BaseGenerationOperator):
    """
    Ensemble Generator æ“ä½œå™¨ï¼ˆé›†æˆç”Ÿæˆï¼‰

    åŠŸèƒ½ï¼š
    - ä½¿ç”¨å¤šä¸ª LLM ç”Ÿæˆç­”æ¡ˆ
    - èåˆå¤šä¸ªå›ç­”
    - æé«˜ç­”æ¡ˆè´¨é‡å’Œé²æ£’æ€§

    åº”ç”¨åœºæ™¯ï¼š
    - é«˜è´¨é‡è¦æ±‚
    - éœ€è¦å¤šæ ·æ€§
    - å…³é”®ä»»åŠ¡
    """

    def __init__(self, config: Dict[str, Any] = None):
        super().__init__(config)
        self.models = self.config.get("models", ["qwen-plus", "qwen-max"])
        self.temperature = self.config.get("temperature", 0.7)
        self.fusion_strategy = self.config.get("fusion_strategy", "voting")  # voting æˆ– concatenate

        # åˆå§‹åŒ–å¤šä¸ª LLM
        self.llms = [
            ChatQwen(model=model, temperature=self.temperature)
            for model in self.models
        ]

    def execute(self, query: str, context: List[Document] = None, **kwargs) -> str:
        """
        é›†æˆç”Ÿæˆç­”æ¡ˆ

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context: ä¸Šä¸‹æ–‡æ–‡æ¡£
            **kwargs: é¢å¤–å‚æ•°

        Returns:
            èåˆåçš„ç­”æ¡ˆ
        """
        prompt_text = kwargs.get("prompt", self._build_default_prompt(query, context))

        prompt_template = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚"),
            ("human", prompt_text)
        ])

        print(f"ğŸ”„ ä½¿ç”¨ {len(self.llms)} ä¸ªæ¨¡å‹ç”Ÿæˆç­”æ¡ˆ...")

        # ä»æ¯ä¸ªæ¨¡å‹ç”Ÿæˆç­”æ¡ˆ
        answers = []
        for i, llm in enumerate(self.llms, 1):
            try:
                chain = prompt_template | llm | StrOutputParser()
                answer = chain.invoke({})
                answers.append(answer.strip())
                print(f"   âœ“ æ¨¡å‹ {i} å®Œæˆ")
            except Exception as e:
                print(f"   âœ— æ¨¡å‹ {i} å¤±è´¥: {e}")

        if not answers:
            return "æŠ±æ­‰ï¼Œæ‰€æœ‰æ¨¡å‹éƒ½ç”Ÿæˆå¤±è´¥ã€‚"

        # èåˆç­”æ¡ˆ
        if self.fusion_strategy == "voting":
            # ç®€å•çš„æŠ•ç¥¨ï¼šè¿”å›æœ€é•¿çš„ç­”æ¡ˆï¼ˆå‡è®¾æ›´è¯¦ç»†ï¼‰
            final_answer = max(answers, key=len)
        elif self.fusion_strategy == "concatenate":
            # è¿æ¥æ‰€æœ‰ç­”æ¡ˆ
            final_answer = self._concatenate_answers(answers)
        else:
            # é»˜è®¤è¿”å›ç¬¬ä¸€ä¸ª
            final_answer = answers[0]

        print(f"âœ… é›†æˆå®Œæˆ")

        return final_answer

    def _build_default_prompt(self, query: str, context: List[Document]) -> str:
        """æ„å»ºé»˜è®¤æç¤º"""
        if context:
            context_text = "\n\n".join([f"[æ–‡æ¡£ {i+1}]\n{doc.page_content}" for i, doc in enumerate(context)])
            return f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ï¼š

ä¸Šä¸‹æ–‡ï¼š
{context_text}

é—®é¢˜ï¼š{query}

ç­”æ¡ˆï¼š"""
        else:
            return f"é—®é¢˜ï¼š{query}\n\nç­”æ¡ˆï¼š"

    def _concatenate_answers(self, answers: List[str]) -> str:
        """è¿æ¥å¤šä¸ªç­”æ¡ˆ"""
        combined = "ç»¼åˆå¤šä¸ªæ¨¡å‹çš„å›ç­”ï¼š\n\n"

        for i, answer in enumerate(answers, 1):
            combined += f"æ¨¡å‹ {i}ï¼š\n{answer}\n\n"

        combined += "ç»¼åˆç»“è®ºï¼š\n"
        # ç®€å•åœ°è¿”å›æœ€é•¿çš„ç­”æ¡ˆä½œä¸ºç»¼åˆç»“è®º
        combined += max(answers, key=len)

        return combined


class AdaptiveGeneratorOperator(BaseGenerationOperator):
    """
    Adaptive Generator æ“ä½œå™¨ï¼ˆè‡ªé€‚åº”ç”Ÿæˆï¼‰

    åŠŸèƒ½ï¼š
    - æ ¹æ®æŸ¥è¯¢å¤æ‚åº¦é€‰æ‹©æ¨¡å‹
    - åŠ¨æ€è°ƒæ•´ç”Ÿæˆå‚æ•°
    - ä¼˜åŒ–æˆæœ¬å’Œè´¨é‡

    åº”ç”¨åœºæ™¯ï¼š
    - éœ€è¦å¹³è¡¡æˆæœ¬å’Œè´¨é‡
    - æŸ¥è¯¢å¤æ‚åº¦ä¸ä¸€
    - èµ„æºä¼˜åŒ–
    """

    def __init__(self, config: Dict[str, Any] = None):
        super().__init__(config)
        self.simple_model = self.config.get("simple_model", "qwen-turbo")
        self.complex_model = self.config.get("complex_model", "qwen-max")
        self.complexity_threshold = self.config.get("complexity_threshold", 0.6)

        # åˆå§‹åŒ–ä¸¤ä¸ªæ¨¡å‹
        self.simple_llm = ChatQwen(model=self.simple_model, temperature=0.7)
        self.complex_llm = ChatQwen(model=self.complex_model, temperature=0.7)

    def execute(self, query: str, context: List[Document] = None, **kwargs) -> str:
        """
        è‡ªé€‚åº”ç”Ÿæˆç­”æ¡ˆ

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context: ä¸Šä¸‹æ–‡æ–‡æ¡£
            **kwargs: é¢å¤–å‚æ•°

        Returns:
            ç”Ÿæˆçš„ç­”æ¡ˆ
        """
        # è¯„ä¼°æŸ¥è¯¢å¤æ‚åº¦
        complexity = self._assess_complexity(query, context)

        # é€‰æ‹©æ¨¡å‹
        if complexity >= self.complexity_threshold:
            llm = self.complex_llm
            model_type = "å¤æ‚æ¨¡å‹"
        else:
            llm = self.simple_llm
            model_type = "ç®€å•æ¨¡å‹"

        print(f"ğŸ“Š æŸ¥è¯¢å¤æ‚åº¦: {complexity:.2f}")
        print(f"ğŸ¤– é€‰æ‹©æ¨¡å‹: {model_type}")

        # ç”Ÿæˆç­”æ¡ˆ
        prompt_text = kwargs.get("prompt", self._build_default_prompt(query, context))

        prompt_template = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚"),
            ("human", prompt_text)
        ])

        try:
            chain = prompt_template | llm | StrOutputParser()
            answer = chain.invoke({})
            return answer.strip()
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}"

    def _assess_complexity(self, query: str, context: List[Document]) -> float:
        """
        è¯„ä¼°æŸ¥è¯¢å¤æ‚åº¦

        Args:
            query: æŸ¥è¯¢
            context: ä¸Šä¸‹æ–‡

        Returns:
            å¤æ‚åº¦åˆ†æ•°ï¼ˆ0-1ï¼‰
        """
        complexity = 0.0

        # æŸ¥è¯¢é•¿åº¦
        if len(query) > 100:
            complexity += 0.3
        elif len(query) > 50:
            complexity += 0.2
        else:
            complexity += 0.1

        # ä¸Šä¸‹æ–‡æ•°é‡
        if context and len(context) > 5:
            complexity += 0.3
        elif context and len(context) > 2:
            complexity += 0.2
        else:
            complexity += 0.1

        # å¤æ‚å…³é”®è¯
        complex_keywords = ["æ¯”è¾ƒ", "åˆ†æ", "è¯„ä¼°", "ç»¼åˆ", "è¯¦ç»†", "è§£é‡Š", "ä¸ºä»€ä¹ˆ"]
        if any(kw in query for kw in complex_keywords):
            complexity += 0.3

        return min(complexity, 1.0)

    def _build_default_prompt(self, query: str, context: List[Document]) -> str:
        """æ„å»ºé»˜è®¤æç¤º"""
        if context:
            context_text = "\n\n".join([f"[æ–‡æ¡£ {i+1}]\n{doc.page_content}" for i, doc in enumerate(context)])
            return f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ï¼š

ä¸Šä¸‹æ–‡ï¼š
{context_text}

é—®é¢˜ï¼š{query}

ç­”æ¡ˆï¼š"""
        else:
            return f"é—®é¢˜ï¼š{query}\n\nç­”æ¡ˆï¼š"
